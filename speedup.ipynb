{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "567d89ee",
   "metadata": {},
   "source": [
    "# Load modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0da92300",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from FNORUNet_5layer_model import *\n",
    "\n",
    "import operator\n",
    "from functools import reduce\n",
    "from functools import partial\n",
    "\n",
    "from timeit import default_timer\n",
    "import scipy.io\n",
    "import math\n",
    "import gc\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf57a16",
   "metadata": {},
   "source": [
    "# Load data (will take a while) and process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e5d8b04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint 1\n"
     ]
    }
   ],
   "source": [
    "hf_r = h5py.File(f'/scratch/groups/smbenson/shale_clean.hdf5', 'r')\n",
    "data_x_shale = np.array(hf_r.get('x'))\n",
    "data_SG_shale = np.array(hf_r.get('SG'))\n",
    "data_P_shale = np.array(hf_r.get('P'))\n",
    "data_P_init_shale = np.array(hf_r.get('P_init'))\n",
    "hf_r.close()\n",
    "print('Checkpoint 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b55435a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019367479650039064\n",
      "0.10341674964334545\n",
      "Checkpoint 2\n",
      "Checkpoint 3\n",
      "Checkpoint 4\n",
      "Checkpoint 5\n",
      "Checkpoint 6\n"
     ]
    }
   ],
   "source": [
    "# GAS SATURATION DATA -------------------------------------------------\n",
    "# Z SCORE NORMALIZATION\n",
    "SG_mean = np.mean(data_SG_shale)\n",
    "SG_std = np.std(data_SG_shale)\n",
    "print(SG_mean)\n",
    "print(SG_std)\n",
    "    \n",
    "data_SG_shale = (data_SG_shale - SG_mean)/(SG_std)\n",
    "\n",
    "data_x = np.concatenate([data_x_shale], axis=0)\n",
    "data_sg = np.concatenate([data_SG_shale], axis=0)\n",
    "\n",
    "data_nr = data_x.shape[0]\n",
    "test_nr = 600\n",
    "train_nr = data_nr - test_nr\n",
    "\n",
    "np.random.seed(0)\n",
    "shuffle_index = np.random.choice(data_nr, data_nr, replace=False)\n",
    "\n",
    "data_x = data_x[shuffle_index, ...]\n",
    "data_sg = data_sg[shuffle_index, ...]\n",
    "\n",
    "print('Checkpoint 2')\n",
    "\n",
    "idx = [0,6,12,18,19,20,21,22,23]\n",
    "data_x_fit = np.zeros((data_x.shape[0], len(idx)+3, 96, 200))\n",
    "for j, index in enumerate(idx):\n",
    "    data_x_fit[:,j,:,:] = data_x[:,index,:,:]\n",
    "    \n",
    "print('Checkpoint 3')\n",
    "    \n",
    "dz = 2.083330\n",
    "dx = [0.1]\n",
    "\n",
    "with open('DRV.txt') as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split('*')\n",
    "        dx.append(float(line[-1]))\n",
    "dx = np.cumsum(dx)\n",
    "grid_x = dx/np.max(dx)\n",
    "grid_x = grid_x[1:]\n",
    "grid_y = np.linspace(0, 200, 96)/np.max(dx)\n",
    "\n",
    "data_x_fit[:,-3,:,:] = grid_x[np.newaxis, np.newaxis, :]\n",
    "data_x_fit[:,-2,:,:] = grid_y[np.newaxis, :, np.newaxis]\n",
    "data_x_fit[:,-1,:,:] = np.ones(data_x_fit[:,-1,:,:].shape)\n",
    "\n",
    "data_x_fit[:,-3,:,:] = data_x_fit[:,-3,:,:]/np.max(data_x_fit[:,-3,:,:])\n",
    "data_x_fit[:,-2,:,:] = data_x_fit[:,-2,:,:]/np.max(data_x_fit[:,-2,:,:])\n",
    "\n",
    "x_in = data_x_fit.transpose((0,2,3,1))\n",
    "SG = data_sg.transpose((0,2,3,1))\n",
    "\n",
    "x_in = x_in.astype(np.float32)\n",
    "SG = SG.astype(np.float32)\n",
    "\n",
    "x_in = torch.from_numpy(x_in)\n",
    "SG = torch.from_numpy(SG)\n",
    "\n",
    "print('Checkpoint 4')\n",
    "\n",
    "# a input u output\n",
    "train_a = x_in[:train_nr,:,:,:]\n",
    "train_u = SG[:train_nr,:,:,:]\n",
    "\n",
    "test_a = x_in[train_nr:train_nr+ test_nr,:,:,:]\n",
    "test_u = SG[train_nr:train_nr+ test_nr,:,:,:]\n",
    "\n",
    "T = 24\n",
    "\n",
    "train_a = train_a[:,:,:,np.newaxis,:]\n",
    "test_a = test_a[:,:,:,np.newaxis,:]\n",
    "\n",
    "train_a = train_a.repeat([1,1,1,T,1])\n",
    "test_a = test_a.repeat([1,1,1,T,1])\n",
    "\n",
    "print('Checkpoint 5')\n",
    "\n",
    "t = np.cumsum(np.power(1.421245, range(24)))\n",
    "t /= np.max(t)\n",
    "for i in range(24):\n",
    "    train_a[:,:,:,i,-1] = t[i]\n",
    "    test_a[:,:,:,i,-1] = t[i]\n",
    "    \n",
    "train_a_SG = train_a\n",
    "test_a_SG = test_a\n",
    "train_u_SG = train_u\n",
    "test_u_SG = test_u\n",
    "    \n",
    "print('Checkpoint 6')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87c9f6e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1323901246254815\n",
      "9.17773280445466\n",
      "Checkpoint 2\n",
      "Checkpoint 3\n",
      "Checkpoint 4\n",
      "Checkpoint 5\n",
      "Checkpoint 6\n"
     ]
    }
   ],
   "source": [
    "# PRESSURE BUILDUP ---------------------------------------------------\n",
    "data_dP_shale = data_P_shale - data_P_init_shale\n",
    "\n",
    "# Z-Score Normalization\n",
    "dP_mean = np.mean(data_dP_shale)\n",
    "dP_std = np.std(data_dP_shale)\n",
    "print(dP_mean)\n",
    "print(dP_std)\n",
    "\n",
    "data_dP_shale = (data_dP_shale - dP_mean)/(dP_std)\n",
    "\n",
    "data_x = np.concatenate([data_x_shale], axis=0)\n",
    "data_dP = np.concatenate([data_dP_shale], axis=0)\n",
    "\n",
    "data_nr = data_x.shape[0]\n",
    "test_nr = 100\n",
    "train_nr = data_nr - test_nr\n",
    "\n",
    "np.random.seed(0)\n",
    "shuffle_index = np.random.choice(data_nr, data_nr, replace=False)\n",
    "\n",
    "data_x = data_x[shuffle_index, ...]\n",
    "data_dP = data_dP[shuffle_index, ...]\n",
    "\n",
    "print('Checkpoint 2')\n",
    "\n",
    "idx = [0,6,12,18,19,20,21,22,23]\n",
    "data_x_fit = np.zeros((data_x.shape[0], len(idx)+3, 96, 200))\n",
    "for j, index in enumerate(idx):\n",
    "    data_x_fit[:,j,:,:] = data_x[:,index,:,:]\n",
    "\n",
    "print('Checkpoint 3')\n",
    "    \n",
    "dz = 2.083330\n",
    "dx = [0.1]\n",
    "\n",
    "with open('DRV.txt') as f:\n",
    "    for line in f:\n",
    "        line = line.strip().split('*')\n",
    "        dx.append(float(line[-1]))\n",
    "dx = np.cumsum(dx)\n",
    "grid_x = dx/np.max(dx)\n",
    "grid_x = grid_x[1:]\n",
    "grid_y = np.linspace(0, 200, 96)/np.max(dx)\n",
    "\n",
    "data_x_fit[:,-3,:,:] = grid_x[np.newaxis, np.newaxis, :]\n",
    "data_x_fit[:,-2,:,:] = grid_y[np.newaxis, :, np.newaxis]\n",
    "data_x_fit[:,-1,:,:] = np.ones(data_x_fit[:,-1,:,:].shape)\n",
    "\n",
    "data_x_fit[:,-3,:,:] = data_x_fit[:,-3,:,:]/np.max(data_x_fit[:,-3,:,:])\n",
    "data_x_fit[:,-2,:,:] = data_x_fit[:,-2,:,:]/np.max(data_x_fit[:,-2,:,:])\n",
    "\n",
    "x_in = data_x_fit.transpose((0,2,3,1))\n",
    "dP = data_dP.transpose((0,2,3,1))\n",
    "\n",
    "x_in = x_in.astype(np.float32)\n",
    "dP = dP.astype(np.float32)\n",
    "\n",
    "x_in = torch.from_numpy(x_in)\n",
    "dP = torch.from_numpy(dP)\n",
    "\n",
    "print('Checkpoint 4')\n",
    "\n",
    "# [3616, 96, 200, 12] = [num reservoirs, vertical length, lateral length, 12 parameters]\n",
    "# [3616, 96, 200, 12] = [..., ..., ..., 24 time steps for prediction]\n",
    "\n",
    "# a input u output\n",
    "train_a = x_in[:train_nr,:,:,:]\n",
    "train_u = dP[:train_nr,:,:,:]\n",
    "\n",
    "test_a = x_in[train_nr:train_nr+ test_nr,:,:,:]\n",
    "test_u = dP[train_nr:train_nr+ test_nr,:,:,:]\n",
    "\n",
    "T = 24\n",
    "\n",
    "train_a = train_a[:,:,:,np.newaxis,:]\n",
    "test_a = test_a[:,:,:,np.newaxis,:]\n",
    "\n",
    "train_a = train_a.repeat([1,1,1,T,1])\n",
    "test_a = test_a.repeat([1,1,1,T,1])\n",
    "\n",
    "print('Checkpoint 5')\n",
    "\n",
    "t = np.cumsum(np.power(1.421245, range(24)))\n",
    "t /= np.max(t)\n",
    "for i in range(24):\n",
    "    train_a[:,:,:,i,-1] = t[i]\n",
    "    test_a[:,:,:,i,-1] = t[i]\n",
    "    \n",
    "train_a_dP = train_a\n",
    "test_a_dP = test_a\n",
    "train_u_dP = train_u\n",
    "test_u_dP = test_u\n",
    "\n",
    "print('Checkpoint 6')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df91816d",
   "metadata": {},
   "source": [
    "# Load gas saturation model and predict 500 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f816b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_or_cpu = 'gpu'\n",
    "normMethod = \"zscore\"\n",
    "\n",
    "# Set torch device\n",
    "device = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12638c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.69571893301327  seconds to predict  500  samples of SG model\n"
     ]
    }
   ],
   "source": [
    "n = 500\n",
    "eval_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(train_a_SG[:n, ...], train_u_SG[:n, ...]), batch_size=1, shuffle=False)\n",
    "\n",
    "from FNORUNet_5layer_model import *\n",
    "model1 = torch.load('/scratch/users/andchu/FNOUNet/saved_models/SG3d_FNORUNet_199ep_32width_12m1_12m2_12m3_3000train_200eps_l2err_0.0005lr_1zerr_zscorenorm_andrew_FNORUNet4_5layer', map_location=torch.device('cuda:0'))\n",
    "\n",
    "t1 = default_timer()\n",
    "with torch.no_grad():\n",
    "    for x, y in eval_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model1(x).view(-1,96,200,24)\n",
    "        \n",
    "t2 = default_timer()\n",
    "print(t2 - t1, \" seconds to predict \", n, \" samples of SG model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ba07ef22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07139143786602653"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time (s) per prediction:\n",
    "35.69571893301327 / 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd8325c",
   "metadata": {},
   "source": [
    "# Load pressure buildup model and predict 500 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c278d5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.42577957699541  seconds to predict  500  samples of dP model\n"
     ]
    }
   ],
   "source": [
    "n = 500\n",
    "eval_loader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(train_a_dP[:n, ...], train_u_dP[:n, ...]), batch_size=1, shuffle=False)\n",
    "\n",
    "from FNORUNet_4layer_model import *\n",
    "model2 = torch.load('/scratch/users/andchu/FNOUNet/saved_models/dP3d_FNORUNet_249ep_32width_12m1_12m2_12m3_3000train_250eps_l1err_0.0005lr_1zerr_zscorenorm_andrew_FNORUNet4_4layer', map_location=torch.device('cuda:0'))\n",
    "\n",
    "\n",
    "t1 = default_timer()\n",
    "with torch.no_grad():\n",
    "    for x, y in eval_loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        pred = model2(x).view(-1,96,200,24)\n",
    "        \n",
    "t2 = default_timer()\n",
    "print(t2 - t1, \" seconds to predict \", n, \" samples of dP model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efe50711",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07085155915399081"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Time (s) per prediction:\n",
    "35.42577957699541 / 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6757107c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8468.409265291435"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# order of magnitude speed-up for pressure buildup model\n",
    "10 * 60 / (35.42577957699541 / 500) # 10 min / prediction time\n",
    "# ~ 1 x 10^4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c40711",
   "metadata": {},
   "source": [
    "# See number of parameters for each model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917aea4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SG FINAL\n",
    "#model = torch.load('/scratch/users/andchu/FNOUNet/saved_models/SG3d_FNORUNet_199ep_32width_12m1_12m2_12m3_3000train_200eps_l2err_0.0005lr_1zerr_zscorenorm_andrew_FNORUNet4_5layer', map_location=torch.device('cuda:0'))\n",
    "\n",
    "# dP FINAL\n",
    "# model = torch.load('/scratch/users/andchu/FNOUNet/saved_models/dP3d_FNORUNet_249ep_32width_12m1_12m2_12m3_3000train_250eps_l1err_0.0005lr_1zerr_zscorenorm_andrew_FNORUNet4_4layer', map_location=torch.device('cuda:0'))\n",
    "\n",
    "# SG no derivative errors\n",
    "# model = torch.load('/scratch/users/andchu/FNOUNet/saved_models/SG3d_FNORUNet_99ep_32width_12m1_12m2_12m3_3000train_100eps_l2err_0.0005lr_0zerr_zscorenorm_andrew_FNORUNet4_5layer_0rerr', map_location=torch.device('cuda:0'))\n",
    "\n",
    "# SG only dy/dr\n",
    "# model = torch.load('/scratch/users/andchu/FNOUNet/saved_models/SG3d_FNORUNet_99ep_32width_12m1_12m2_12m3_3512train_100eps_l2err_0.0005lr_0zerr_zscorenorm_andrew_FNORUNet3_5layer', map_location=torch.device('cuda:0'))\n",
    "\n",
    "# dP no derivative errors\n",
    "# model = torch.load('/scratch/users/andchu/FNOUNet/saved_models/dP3d_FNORUNet_99ep_32width_12m1_12m2_12m3_3000train_100eps_l1err_0.0005lr_0zerr_zscorenorm_andrew_FNORUNet4_4layer_0rerr', map_location=torch.device('cuda:0'))\n",
    "\n",
    "# dP only dy/dr\n",
    "model = torch.load('/scratch/users/andchu/FNOUNet/saved_models/dP3d_FNORUNet_99ep_32width_12m1_12m2_12m3_3000train_100eps_l1err_0.0005lr_0zerr_zscorenorm_andrew_FNORUNet4_4layer', map_location=torch.device('cuda:0'))\n",
    "\n",
    "total_params = sum(\n",
    "\tparam.numel() for param in model.parameters()\n",
    ")\n",
    "f'Number of model parameters: {total_params:,}'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
